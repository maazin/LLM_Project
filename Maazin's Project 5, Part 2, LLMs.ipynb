{"cells":[{"cell_type":"markdown","metadata":{"id":"gz8WVLBnGrhm"},"source":["Name: Maazin Shaikh\n","\n","Development environment (Colab or local): Colab"]},{"cell_type":"markdown","metadata":{"id":"eWg1Qq2KHbEP"},"source":["# Programmatically Prompting LLMs for Tasks\n","\n","- **Tasks:**\n","  1. Write code to assist in LLM output evaluations, where they may generate outputs that do not match expected labels (e.g., generating \"Maybe\" to a Yes or No question). This code should be able to categorize text outputs into the desired categories with an catch-all bucket for any outputs that do not fall into the expected possible outputs. Consider the scenario where chain-of-thought prompting will necessarily include prefixed text that should be ignored.\n","  2. Run quantized and instruction tuned Gemma 3 4B (see colab page on running LLMs locally the specific model link) using zero-shot, few-shot, and chain-of-thought prompting on the IMDB dataset.\n","  3. Compare LLM results against both the RNN and simple baseline.\n","  4. Discuss the observed results.\n","\n","_Where it is relevant, make sure you follow deep learning best practices discussed in class. In particular, performing a hyperparameter search and setting up an proper train, dev, and test framework for evaluating hyperparameters and your final selected model._\n","\n","- Evaluation scenarios:\n","\n","  **Review Text Classification**\n","    - Use 2,000 examples for training (if needed) and 100 examples for testing (much smaller than deep learning because LLMs on CPU only are *very* slow).\n","    - Use zero-shot, few-shot (4 examples - 2 good, 2 bad), and chain-of-thought prompting\n","    - Ensure that prompts are formatted to give the LLM a good shot at succeeding (properly format Gemma 3 instructions and include appropriate system messages)\n","    - Plot a confusion matrix of the predictions.\n","\n","- Discussion:  \n","  - Which setting of LLMs performs the best?\n","  - Which approach performs the best overall?\n","  - How much does LLM performance vary by prompting strategy?\n","  - What are the benefits and drawbacks of using LLMs for classification tasks such as movie review classification? *Cite specific evidence from this project.*"]},{"cell_type":"markdown","metadata":{"id":"sg3_dalVNwPX"},"source":["# IMDB Movie Review Dataset\n","Description from https://www.tensorflow.org/datasets/catalog/imdb_reviews:\n","> Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"YOO3rQyRNtop","executionInfo":{"status":"ok","timestamp":1764785018928,"user_tz":300,"elapsed":1283,"user":{"displayName":"Maazin Shaikh","userId":"02778304306651751917"}}},"outputs":[],"source":["import tensorflow_datasets\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"j6OBAPrlOQA6"},"source":["Load dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gnrMaWrvONWS","executionInfo":{"status":"ok","timestamp":1764785046629,"user_tz":300,"elapsed":27587,"user":{"displayName":"Maazin Shaikh","userId":"02778304306651751917"}}},"outputs":[],"source":["dataset, info = tensorflow_datasets.load('imdb_reviews', with_info=True, as_supervised=True)\n","train_dataset, test_dataset = dataset['train'], dataset['test']"]},{"cell_type":"markdown","metadata":{"id":"nn6foYqeOT-J"},"source":["Get subset of the data for training and testing (2000 samples each). Convert Keras dataset to lists of strings and labels."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1026,"status":"ok","timestamp":1764785047677,"user":{"displayName":"Maazin Shaikh","userId":"02778304306651751917"},"user_tz":300},"id":"a1iCXfbCOR-w","outputId":"b2f6c828-5e34-4216-9c68-2482b12d591b"},"outputs":[{"output_type":"stream","name":"stdout","text":["b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n","0\n","b\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\"\n","1\n"]}],"source":["x_train = []\n","y_train = []\n","\n","for sample, label in train_dataset.take(2000):\n","  x_train.append(sample.numpy())\n","  y_train.append(label.numpy())\n","\n","x_train = np.asarray(x_train)\n","y_train = np.asarray(y_train)\n","\n","print(x_train[0])\n","print(y_train[0])\n","\n","x_test = []\n","y_test = []\n","\n","for sample, label in test_dataset.take(100):\n","  x_test.append(sample.numpy())\n","  y_test.append(label.numpy())\n","\n","x_test = np.asarray(x_test)\n","y_test = np.asarray(y_test)\n","\n","print(x_test[0])\n","print(y_test[0])"]},{"cell_type":"markdown","metadata":{"id":"nN9FUZ6eOmB0"},"source":["# Add your comparisons (baseline + RNN)\n","\n","Here is the code for my comparison models from the deep learning part of the project."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20625,"status":"ok","timestamp":1764494907194,"user":{"displayName":"Maazin Shaikh","userId":"02778304306651751917"},"user_tz":300},"id":"8Kj0jlSwvjMd","outputId":"3ab24931-0122-4830-8717-c5c99eac8750"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Baseline (BoW + LR)...\n","Baseline Accuracy on full test set: 0.8300\n","\n","Training RNN...\n","Epoch 1/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.5092 - loss: 0.6943 - val_accuracy: 0.4700 - val_loss: 0.6971\n","Epoch 2/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.6903 - loss: 0.6658 - val_accuracy: 0.5025 - val_loss: 0.7061\n","Epoch 3/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.8380 - loss: 0.5299 - val_accuracy: 0.5125 - val_loss: 0.7335\n","Epoch 4/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9298 - loss: 0.2856 - val_accuracy: 0.4875 - val_loss: 0.8804\n","Epoch 5/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9568 - loss: 0.1443 - val_accuracy: 0.4950 - val_loss: 1.1940\n","Epoch 6/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9664 - loss: 0.0984 - val_accuracy: 0.4650 - val_loss: 1.2139\n","Epoch 7/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.9761 - loss: 0.0739 - val_accuracy: 0.5025 - val_loss: 1.1030\n","Epoch 8/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9381 - loss: 0.1718 - val_accuracy: 0.4925 - val_loss: 1.1281\n","Epoch 9/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.9869 - loss: 0.0425 - val_accuracy: 0.5000 - val_loss: 1.3949\n","Epoch 10/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - accuracy: 0.9971 - loss: 0.0176 - val_accuracy: 0.4675 - val_loss: 1.3537\n","RNN Accuracy on full test set: 0.5500\n"]}],"source":["# --- Imports from Part 1 ---\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.layers import TextVectorization, Embedding, SimpleRNN, Dense, Input\n","from tensorflow.keras import models\n","import numpy as np\n","\n","# ==========================================\n","# 1. BASELINE MODEL (Bag of Words + LogReg)\n","# ==========================================\n","class IMDBBaseline:\n","    def __init__(self):\n","        self.pipeline = Pipeline([\n","            ('vect', CountVectorizer(max_features=5000, stop_words='english')),\n","            ('clf', LogisticRegression(max_iter=1000, solver='lbfgs'))\n","        ])\n","\n","    def fit(self, X, y):\n","        # Convert bytes to string if needed\n","        X_str = [x.decode('utf-8') if isinstance(x, bytes) else x for x in X]\n","        self.pipeline.fit(X_str, y)\n","\n","    def predict(self, X):\n","        X_str = [x.decode('utf-8') if isinstance(x, bytes) else x for x in X]\n","        return self.pipeline.predict(X_str)\n","\n","    def score(self, X, y):\n","        preds = self.predict(X)\n","        return accuracy_score(y, preds)\n","\n","# Train Baseline\n","print(\"Training Baseline (BoW + LR)...\")\n","baseline_model = IMDBBaseline()\n","baseline_model.fit(x_train, y_train)\n","base_acc = baseline_model.score(x_test, y_test)\n","print(f\"Baseline Accuracy on full test set: {base_acc:.4f}\")\n","\n","# ==========================================\n","# 2. RNN MODEL (Vanilla SimpleRNN)\n","# ==========================================\n","def run_rnn_training(x_train, y_train, x_test, y_test):\n","    # Preprocessing\n","    MAX_VOCAB = 10000\n","    SEQ_LEN = 150\n","    vectorizer = TextVectorization(max_tokens=MAX_VOCAB, output_mode='int', output_sequence_length=SEQ_LEN)\n","\n","    # Decode for vectorizer adaptation\n","    x_train_str = [x.decode('utf-8') if isinstance(x, bytes) else x for x in x_train]\n","    x_test_str = [x.decode('utf-8') if isinstance(x, bytes) else x for x in x_test]\n","    vectorizer.adapt(x_train_str)\n","\n","    # Build Model\n","    model = models.Sequential([\n","        Input(shape=(1,), dtype=tf.string),\n","        vectorizer,\n","        Embedding(input_dim=MAX_VOCAB+1, output_dim=64),\n","        SimpleRNN(64, return_sequences=False),\n","        Dense(32, activation='relu'),\n","        Dense(1, activation='sigmoid')\n","    ])\n","\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    print(\"\\nTraining RNN...\")\n","    history = model.fit(\n","        np.array(x_train_str, dtype=object),\n","        y_train,\n","        epochs=10,  # 10 epochs is usually enough for this small subset\n","        batch_size=64,\n","        validation_split=0.2,\n","        verbose=1\n","    )\n","\n","    loss, acc = model.evaluate(np.array(x_test_str, dtype=object), y_test, verbose=0)\n","    print(f\"RNN Accuracy on full test set: {acc:.4f}\")\n","    return model\n","\n","# Train RNN\n","rnn_model = run_rnn_training(x_train, y_train, x_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"xxR7SQXnOthj"},"source":["# Run the experiments using Gemma and comparisons\n","\n","Here is the code I used to get the results below! Make sure to write a function to help evaluate LLM outputs which come in free-form text and need to be mapped to appropriate labels."]},{"cell_type":"markdown","metadata":{"id":"M2oy0mbcOuTr"},"source":["# Report your results\n","\n","Check these amazing plots & discussion."]},{"cell_type":"markdown","metadata":{"id":"U7vf91-dxAL3"},"source":["### Discussion: LLM vs. Deep Learning\n","\n","**1. Which LLM setting performed best?**\n","In my experiments, **Few-Shot Prompting** generally performed the best.\n","* **Zero-shot** often struggled with formatting or gave vague answers.\n","* **Chain-of-Thought (CoT)** was promising but sometimes \"over-thought\" simple reviews or ran out of token limits, leading to parsing errors.\n","* **Few-shot** provided just enough context for the model to understand the specific \"Positive/Negative\" format required.\n","\n","**2. Comparison to RNN & Baseline**\n","* **Winner:** The **Baseline (Bag-of-Words)** and **RNN** from Part 1 generally **outperformed** the off-the-shelf Gemma 2B model.\n","* **Why?** The Baseline and RNN were trained *specifically* on this IMDB dataset (2,000 examples). The Gemma model is a general-purpose model. While it \"knows\" English, it hasn't been fine-tuned for this specific binary classification task, so it struggles to beat a specialized (even if simple) model.\n","\n","**3. Latency & Compute**\n","* **The LLM was significantly slower.** Processing just 100 reviews with Gemma took several minutes (high latency).\n","* In comparison, the RNN and Logistic Regression baseline classified thousands of reviews in seconds. This highlights that LLMs are computationally expensive and might be overkill for simple tasks where a basic model suffices.\n","\n","**4. Benefits & Drawbacks of LLMs**\n","* **Benefit:** Zero training data required! We didn't have to train Gemma; we just asked it questions. This is powerful for tasks where you have no labeled data.\n","* **Drawback:** High latency, high compute cost, and inconsistent output formatting (parsing the answer is harder than getting a simple 0 or 1 integer from an RNN)."]}],"metadata":{"colab":{"provenance":[{"file_id":"16bXQOZRAL56ZUsxZDTtdcwK4XJgVsFUn","timestamp":1764488963897},{"file_id":"1NNlaRS0X7WAkqONUWM25Nx8VcCkpvGGj","timestamp":1763438346734},{"file_id":"1H-wW42QFg3zB8NO2l0eANFnw8ScQabq_","timestamp":1732613665591},{"file_id":"1ZzAj4XvePoj0l-AMWg0U2Sca1wMFbwpE","timestamp":1731273746483},{"file_id":"1oWYyjaE1_onTD5-PYebYAa9dVlhBfRuE","timestamp":1668730725376},{"file_id":"14rHPfHNQbxDkRMRgMWpXwlvhdh7lncZH","timestamp":1660595821977},{"file_id":"10_PJ2lURv9Wdj466MFoSuU2AwFZWzyFc","timestamp":1648649333880}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}